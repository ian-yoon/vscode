{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes')) # 헤더, 푸터, 문장부호 삭제\n",
    "documents = dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/96/h5kgn7_s0nd40p92psr15zbm0000gn/T/ipykernel_18528/943881275.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \") # 알파벳만 두고 나머지는 공백으로\n"
     ]
    }
   ],
   "source": [
    "news_df = pd.DataFrame({'document': documents})\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \") # 알파벳만 두고 나머지는 공백으로\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3])) # 단어에 글자수가 4글자 이상인 것만 놔둠\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower()) # 단어를 전부 소문자로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 공백 기준으로 단어를 다 나눔\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words]) # 불용어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenized_doc = []\n",
    "for i in range(len(news_df)):\n",
    "    t = ' '.join(tokenized_doc[i]) # 공백을 기준으로 나뉘어있던 단어를 다시 공백을 통해 조인\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "news_df['clean_doc'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000) # 전체 단어에 대해서 다 하는 게 아니라 단어 수 상위 1000개만\n",
    "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd_model = TruncatedSVD(n_components=20) # 행렬 특이값 분해. 차원 축소 개념. 11314개의 행을 20개로 압축\n",
    "svd_model.fit(X)\n",
    "len(svd_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.shape(svd_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:  [('like', 0.21386), ('know', 0.20046), ('people', 0.19293), ('think', 0.17805), ('good', 0.15128)]\n",
      "Topic 2:  [('thanks', 0.32891), ('windows', 0.29072), ('card', 0.18078), ('drive', 0.17451), ('mail', 0.1511)]\n",
      "Topic 3:  [('game', 0.37063), ('team', 0.32403), ('year', 0.28147), ('games', 0.25328), ('season', 0.18408)]\n",
      "Topic 4:  [('drive', 0.53088), ('scsi', 0.20404), ('hard', 0.15551), ('disk', 0.15503), ('card', 0.14439)]\n",
      "Topic 5:  [('windows', 0.40909), ('file', 0.25523), ('window', 0.17776), ('files', 0.16544), ('program', 0.13993)]\n",
      "Topic 6:  [('mail', 0.16663), ('government', 0.15703), ('chip', 0.14934), ('space', 0.14539), ('information', 0.13861)]\n",
      "Topic 7:  [('like', 0.66955), ('bike', 0.14254), ('know', 0.11227), ('chip', 0.11018), ('sounds', 0.10378)]\n",
      "Topic 8:  [('card', 0.42788), ('sale', 0.22683), ('video', 0.20916), ('monitor', 0.15626), ('price', 0.15573)]\n",
      "Topic 9:  [('know', 0.42763), ('card', 0.34943), ('people', 0.19555), ('chip', 0.16402), ('government', 0.1635)]\n",
      "Topic 10:  [('good', 0.4604), ('know', 0.26032), ('time', 0.17947), ('bike', 0.10556), ('want', 0.09199)]\n",
      "Topic 11:  [('think', 0.7816), ('thanks', 0.14331), ('chip', 0.1181), ('clipper', 0.08291), ('encryption', 0.08038)]\n",
      "Topic 12:  [('know', 0.38569), ('file', 0.19051), ('jesus', 0.18418), ('space', 0.15974), ('files', 0.1086)]\n",
      "Topic 13:  [('people', 0.36966), ('sale', 0.21567), ('windows', 0.20726), ('bike', 0.19329), ('know', 0.19204)]\n",
      "Topic 14:  [('good', 0.40892), ('people', 0.2342), ('chip', 0.22051), ('windows', 0.18622), ('clipper', 0.14591)]\n",
      "Topic 15:  [('right', 0.41055), ('bike', 0.17731), ('sale', 0.17182), ('window', 0.16428), ('offer', 0.13945)]\n",
      "Topic 16:  [('people', 0.45522), ('problem', 0.25996), ('window', 0.24596), ('need', 0.19264), ('time', 0.128)]\n",
      "Topic 17:  [('time', 0.5446), ('file', 0.28853), ('bike', 0.19549), ('need', 0.13534), ('card', 0.12643)]\n",
      "Topic 18:  [('need', 0.40928), ('right', 0.20315), ('want', 0.20026), ('file', 0.18975), ('good', 0.16373)]\n",
      "Topic 19:  [('problem', 0.35983), ('file', 0.31486), ('armenian', 0.16874), ('think', 0.15444), ('armenians', 0.1507)]\n",
      "Topic 20:  [('problem', 0.48653), ('mail', 0.25194), ('year', 0.22571), ('make', 0.11243), ('jesus', 0.10861)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d: \"% (idx+1),[(feature_names[i],topic[i].round(5)) for i in topic.argsort()[: -n -1:-1]])\n",
    "\n",
    "get_topics(svd_model.components_,terms)\n",
    "# 토픽의 단어와 그 단어가 차지하는 중요도가 출력됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 잠재 디리클레 할당 LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "stm = PorterStemmer()\n",
    "stopwords = set(stopwords.words('english'))\n",
    "pattern = re.compile('[a-zA-Z][-_a-zA-Z0-9.]*')\n",
    "\n",
    "def tokenize(sentence):\n",
    "    def stem(w):\n",
    "        try: return stm.stem(w)\n",
    "        except: return w\n",
    "    return [stem(w) for w in word_tokenize(sentence.lower())\n",
    "            if w not in stopwords and pattern.match(w)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp\n",
    "\n",
    "model = tp.LDAModel(k=20, min_cf=5)\n",
    "for i, line in enumerate(open('/Users/ian/Desktop/Study/data/text/trumph.txt', encoding='cp949')):\n",
    "    model.add_doc(tokenize(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs:  1\n",
      "Total words:  162\n",
      "Vocab size:  21\n"
     ]
    }
   ],
   "source": [
    "model.train(0)\n",
    "print('Total docs: ',len(model.docs))\n",
    "print('Total words: ',model.num_words)\n",
    "print('Vocab size: ',model.num_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0\tamerican, peopl, one, back, great, world, everi, protect, countri, nation\n",
      "Topic #1\tamerica, american, nation, countri, peopl, one, everi, protect, world, great\n",
      "Topic #2\tamerica, american, nation, countri, peopl, one, everi, protect, world, great\n",
      "Topic #3\tgod, american, nation, countri, peopl, one, everi, protect, world, great\n",
      "Topic #4\tamerica, american, nation, countri, peopl, one, everi, protect, world, great\n",
      "Topic #5\tcountri, american, nation, america, peopl, one, everi, protect, world, great\n",
      "Topic #6\tdream, right, nation, countri, peopl, one, everi, protect, world, great\n",
      "Topic #7\teveri, new, nation, countri, peopl, one, american, protect, world, great\n",
      "Topic #8\tamerica, american, nation, countri, peopl, one, everi, protect, world, great\n",
      "Topic #9\tamerica, today, new, countri, peopl, one, everi, protect, world, great\n",
      "Topic #10\tnever, make, nation, countri, peopl, one, everi, protect, world, great\n",
      "Topic #11\tnation, across, make, countri, peopl, one, everi, protect, world, great\n",
      "Topic #12\tamerica, american, nation, countri, peopl, one, everi, protect, world, great\n",
      "Topic #13\tamerica, american, nation, countri, peopl, one, everi, protect, world, great\n",
      "Topic #14\tpresid, american, nation, countri, peopl, one, everi, protect, world, great\n",
      "Topic #15\tamerica, american, nation, countri, peopl, one, everi, protect, world, great\n",
      "Topic #16\tamerica, american, nation, countri, peopl, one, everi, protect, world, great\n",
      "Topic #17\tamerica, american, nation, countri, peopl, one, everi, protect, world, great\n",
      "Topic #18\tprotect, mani, nation, countri, peopl, one, everi, american, world, great\n",
      "Topic #19\tamerica, american, nation, countri, peopl, one, everi, protect, world, great\n"
     ]
    }
   ],
   "source": [
    "model.train(200)\n",
    "for i in range(model.k):\n",
    "    res = model.get_topic_words(i, top_n=10)\n",
    "    print('Topic #{0}'.format(i), end='\\t')\n",
    "    print(', '.join(w for w, p in res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "emma_raw = nltk.corpus.gutenberg.raw('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs:  1\n",
      "Total words:  67220\n",
      "Vocab size:  1776\n"
     ]
    }
   ],
   "source": [
    "import tomotopy as tp\n",
    "\n",
    "model = tp.LDAModel(k=5, min_cf=5)\n",
    "model.add_doc(tokenize(emma_raw))\n",
    "\n",
    "model.train(0)\n",
    "print('Total docs: ',len(model.docs))\n",
    "print('Total words: ',model.num_words)\n",
    "print('Vocab size: ',model.num_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0\tknightley, mr.\n",
      "Topic #1\tmr., could\n",
      "Topic #2\tharriet, would\n",
      "Topic #3\talway, one\n",
      "Topic #4\temma, thing\n"
     ]
    }
   ],
   "source": [
    "model.train(100)\n",
    "for i in range(model.k):\n",
    "    res = model.get_topic_words(i, top_n=2)\n",
    "    print('Topic #{0}'.format(i), end='\\t')\n",
    "    print(', '.join(w for w, p in res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs:  27\n",
      "Total words:  320\n",
      "Vocab size:  101\n",
      "Topic #0\t여성, 불안, 취업, 청년들, 선택, 분야, 자리, 서류전형, 이상, 영향\n",
      "Topic #1\t여성들, 노동환경, 가치, 때문, 증명, 경력, 기회, 직장, 좌절, 시간\n",
      "Topic #2\t여성, 참여자, 아이, 팬데믹, 성차별, 자신, 정도, 심층, 인터뷰, 청년\n",
      "Topic #3\t회사, 세대, 전망, 퇴사, 조직문화, 권리, 년대생, 직업적, 보장, 추구\n",
      "Topic #4\t애교, 강요, 사장님, 존재, 거래처, 회식, 직원, 이슈, 거예요, 재롱\n",
      "Topic #5\t이직, 비율, 만원, 년대생, 한번, 이해, 결혼, 심층, 인터뷰, 청년\n",
      "Topic #6\t노동, 여성, 노동자들, 환경, 결혼, 미래, 연구원, 현실, 비전, 고군분투\n",
      "Topic #7\t문제, 청년, 사회, 결과, 진행, 심각, 관계, 응답, 설문조사, 부적응자\n",
      "Topic #8\t인터뷰, 심층, 정규직, 위치, 최혜영, 실태조사, 직장, 경우, 부담, 청년\n",
      "Topic #9\t상황, 분석, 하나, 사람, 참여, 조건, 경제적, 어려움, 사회적, 요구\n"
     ]
    }
   ],
   "source": [
    "import tomotopy as tp\n",
    "import re\n",
    "from konlpy.tag import Hannanum\n",
    "\n",
    "han = Hannanum()\n",
    "\n",
    "model = tp.LDAModel(k=10, min_cf=2)\n",
    "\n",
    "for i, line in enumerate(open('/Users/ian/Desktop/Study/data/text/news1.txt', encoding='utf-8')):\n",
    "    sentence = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z]', ' ', line)\n",
    "    a = sentence.strip()\n",
    "    n = han.nouns(a)\n",
    "    n2 = [x for x in n if len(x)>1]\n",
    "    if len(n2) > 0:\n",
    "        model.add_doc(n2)\n",
    "\n",
    "model.train(0)\n",
    "print('Total docs: ',len(model.docs))\n",
    "print('Total words: ',model.num_words)\n",
    "print('Vocab size: ',model.num_vocabs)\n",
    "\n",
    "model.train(100)\n",
    "for i in range(model.k):\n",
    "    res = model.get_topic_words(i, top_n=10)\n",
    "    print('Topic #{0}'.format(i), end='\\t')\n",
    "    print(', '.join(w for w, p in res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
