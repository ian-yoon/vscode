{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"모처럼 전국에 비가 내리고 있습니다.\n",
    "대부분 밤까지 계속되기 때문에 종일 우산이 필요하겠는데요.\n",
    "비의 양도 많고 바람도 강하게 불기 때문에 작은 우산 말고 큰 우산으로 챙기는 게 더 좋습니다.\n",
    "특히 제주와 남해안에서 비바람이 강합니다.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['모처럼 전국에 비가 내리고 있습니다.', '대부분 밤까지 계속되기 때문에 종일 우산이 필요하겠는데요.', '비의 양도 많고 바람도 강하게 불기 때문에 작은 우산 말고 큰 우산으로 챙기는 게 더 좋습니다.', '특히 제주와 남해안에서 비바람이 강합니다.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = sent_tokenize(text) # 문장 단위로 토큰화\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['모처럼', '전국', '비'],\n",
       " ['대부분', '밤', '계속', '때문', '종일', '우산'],\n",
       " ['비', '양도', '바람', '불기', '때문', '우산', '우산', '게', '더'],\n",
       " ['제주', '남해안', '비바람']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "text2 = []\n",
    "for txt in text:\n",
    "    t = okt.nouns(txt) # 한글 형태소 분석. 명사 추출\n",
    "    text2.append(t)\n",
    "\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['모처럼', '전국', '비', '있습니다'],\n",
       " ['대부분', '밤', '계속', '때문', '종일', '우산', '필요하겠는데요'],\n",
       " ['비',\n",
       "  '양도',\n",
       "  '많고',\n",
       "  '바람',\n",
       "  '강하게',\n",
       "  '불기',\n",
       "  '때문',\n",
       "  '작은',\n",
       "  '우산',\n",
       "  '우산',\n",
       "  '게',\n",
       "  '더',\n",
       "  '좋습니다'],\n",
       " ['제주', '남해안', '비바람', '강합니다']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "text2 = []\n",
    "for txt in text:\n",
    "    morph = okt.pos(txt)\n",
    "    text2.append(morph)\n",
    "\n",
    "text3 = []\n",
    "for text in text2:\n",
    "    line = []\n",
    "    for word, tag in text:\n",
    "        if tag in ['Noun','Adjective']: # 명사와 형용사 같이\n",
    "            line.append(word)\n",
    "    text3.append(line)\n",
    "\n",
    "text3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['모처럼', '전국', '비', '있습니다'],\n",
       " ['대부분', '밤', '계속', '때문', '종일', '우산', '필요하겠는데요'],\n",
       " ['비', '양도', '많고', '바람', '강하게', '불기', '때문', '작은', '우산', '우산', '좋습니다'],\n",
       " ['제주', '남해안', '비바람', '강합니다']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {} # 단어 사전\n",
    "sentences = []\n",
    "stop_words = ['더', '게']\n",
    "\n",
    "for txt in text3:\n",
    "    result = []\n",
    "    for word in txt:\n",
    "        if word not in stop_words:\n",
    "            result.append(word) # 불용어가 아니고,\n",
    "            if word not in vocab:\n",
    "                vocab[word] = 0 # 새로운 단어면 추가\n",
    "            vocab[word] += 1 # 기존에 있던 단어면 카운트 추가\n",
    "    sentences.append(result)\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'모처럼': 1, '전국': 1, '비': 2, '있습니다': 1, '대부분': 1, '밤': 1, '계속': 1, '때문': 2, '종일': 1, '우산': 3, '필요하겠는데요': 1, '양도': 1, '많고': 1, '바람': 1, '강하게': 1, '불기': 1, '작은': 1, '좋습니다': 1, '제주': 1, '남해안': 1, '비바람': 1, '강합니다': 1}\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 빈도수\n",
    "\n",
    "print(vocab)\n",
    "print(vocab[\"우산\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'비': 1, '때문': 2, '우산': 3}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "i = 0\n",
    "for word in vocab:\n",
    "    if vocab[word] > 1:\n",
    "        i = i + 1\n",
    "        word_to_index[word] = i\n",
    "\n",
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 4, 1, 4],\n",
       " [4, 4, 4, 2, 4, 3, 4],\n",
       " [1, 4, 4, 4, 4, 4, 2, 4, 3, 3, 4],\n",
       " [4, 4, 4, 4]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 집합에 없는 단어는 word_to_index에 OOV라는 단어를 추가해서 OOV로 처리\n",
    "# 기타 단어 OOV의 인덱스 4로 설정\n",
    "\n",
    "word_to_index['OOV'] = len(word_to_index) + 1\n",
    "\n",
    "encoded = []\n",
    "for s in sentences:\n",
    "    temp = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            temp.append(word_to_index[w])\n",
    "        except KeyError:\n",
    "            temp.append(word_to_index['OOV'])\n",
    "    encoded.append(temp)\n",
    "\n",
    "encoded\n",
    "\n",
    "# 이 방법이 바로 정수 인코딩\n",
    "# 단어 하나하나에 번호를 지정하는 것\n",
    "# 단점은 가중치가 붙을 수 있다는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['모처럼', '전국', '비', '있습니다', '대부분', '밤', '계속', '때문', '종일', '우산',\n",
       "       '필요하겠는데요', '비', '양도', '많고', '바람', '강하게', '불기', '때문', '작은', '우산',\n",
       "       '우산', '좋습니다', '제주', '남해안', '비바람', '강합니다'], dtype='<U7')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "words = np.hstack(sentences) # 2차원 데이터를 1차원으로\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'우산': 3, '비': 2, '때문': 2, '모처럼': 1, '전국': 1, '있습니다': 1, '대부분': 1, '밤': 1, '계속': 1, '종일': 1, '필요하겠는데요': 1, '양도': 1, '많고': 1, '바람': 1, '강하게': 1, '불기': 1, '작은': 1, '좋습니다': 1, '제주': 1, '남해안': 1, '비바람': 1, '강합니다': 1})\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab = Counter(words) # 단어의 출현빈도를 쉽게 계산하는 클래스\n",
    "print(vocab)\n",
    "print(vocab[\"우산\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('우산', 3), ('비', 2), ('때문', 2), ('모처럼', 1), ('전국', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'우산': 1, '비': 2, '때문': 3, '모처럼': 4, '전국': 5}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab:\n",
    "    i = i + 1\n",
    "    word_to_index[word] = i\n",
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원핫인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '는', '학교', '에', '간다', '나', '는', '집', '에', '간다']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "token = okt.morphs(\"나는 학교에 간다 나는 집에 간다\")\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나': 0, '는': 1, '학교': 2, '에': 3, '간다': 4, '집': 5}\n"
     ]
    }
   ],
   "source": [
    "word2index = {}\n",
    "for idx, voca in enumerate(token):\n",
    "    if voca not in word2index.keys():\n",
    "        word2index[voca] = len(word2index)\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot_encoding(word, word2index):\n",
    "    one_hot_vector = [0]*(len(word2index)) # 전체 단어의 개수만큼 0으로 채운 리스트\n",
    "    index = word2index[word] # 해당하는 단어의 인덱스 찾기\n",
    "    one_hot_vector[index] = 1 # 해당하는 단어만 1, 나머지는 0\n",
    "    return one_hot_vector\n",
    "\n",
    "one_hot_encoding(\"학교\", word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['나', '는', '학교', '에', '간다', '집'])\n",
      "나 [1, 0, 0, 0, 0, 0]\n",
      "는 [0, 1, 0, 0, 0, 0]\n",
      "학교 [0, 0, 1, 0, 0, 0]\n",
      "에 [0, 0, 0, 1, 0, 0]\n",
      "간다 [0, 0, 0, 0, 1, 0]\n",
      "집 [0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# 추가된 코드\n",
    "\n",
    "key_list = word2index.keys()\n",
    "print(key_list)\n",
    "for key in key_list:\n",
    "    print(key, one_hot_encoding(key, word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'비': 0, '가': 1, '오니': 2, '마음': 3, '이': 4, '차분해지네요': 5, '요즘': 6, '너무': 7, '더웠어요': 8, '기쁘네요': 9}\n",
      "[2, 2, 2, 2, 2, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re\n",
    "\n",
    "okt = Okt()\n",
    "token = re.sub(\"[.!#~]\",\"\",\"비가 오니 마음이 차분해지네요. 요즘 너무 더웠어요. 비가 오니 마음이 기쁘네요.\")\n",
    "token = okt.morphs(token)\n",
    "\n",
    "word2index = {}\n",
    "bow = []\n",
    "for voca in token:\n",
    "    if voca not in word2index.keys():\n",
    "        word2index[voca] = len(word2index)\n",
    "        bow.insert(len(word2index)-1,1)\n",
    "    else:\n",
    "        index = word2index.get(voca)\n",
    "        bow[index] = bow[index]+1\n",
    "    \n",
    "print(word2index)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'모처럼': 1, '전국에': 4, '비가': 2, '내리고': 0, '있습니다': 3}\n",
      "[[0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['모처럼 전국에 비가 내리고 있습니다.']\n",
    "line = ['전국에 비가']\n",
    "vector = CountVectorizer()\n",
    "vector.fit(corpus)\n",
    "print(vector.vocabulary_)\n",
    "print(vector.transform(line).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
      "{'모처럼': 9, '전국에': 22, '비가': 13, '내리고': 4, '있습니다': 20, '대부분': 5, '밤까지': 11, '계속되기': 2, '때문에': 6, '종일': 24, '우산이': 19, '필요하겠는데요': 28, '비의': 15, '양도': 16, '많고': 7, '바람도': 10, '강하게': 0, '불기': 12, '작은': 21, '우산': 17, '말고': 8, '우산으로': 18, '챙기는': 26, '좋습니다': 25, '특히': 27, '제주와': 23, '남해안에서': 3, '비바람이': 14, '강합니다': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"\"\"모처럼 전국에 비가 내리고 있습니다.\n",
    "대부분 밤까지 계속되기 때문에 종일 우산이 필요하겠는데요.\n",
    "비의 양도 많고 바람도 강하게 불기 때문에 작은 우산 말고 큰 우산으로 챙기는 게 더 좋습니다.\n",
    "특히 제주와 남해안에서 비바람이 강합니다.\"\"\"]\n",
    "\n",
    "vector = CountVectorizer()\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
