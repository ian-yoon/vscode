{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from gensim.models import word2vec\n",
    "import codecs\n",
    "\n",
    "f = open(\"c:/data/text/ratings.csv\", encoding=\"ms949\")\n",
    "data = f.read()\n",
    "twitter = Okt()\n",
    "lines = data.split(\"\\n\")\n",
    "results = []\n",
    "for line in lines[:1000]:\n",
    "    r = []\n",
    "    word_list = twitter.pos(line, norm=True, stem=True)\n",
    "    for (word, pumsa) in word_list:\n",
    "        if not pumsa in [\"Josa\", \"Eomi\", \"Punctuation\"]:\n",
    "            r.append(word)\n",
    "    results.append((\" \".join(r)).strip())\n",
    "\n",
    "output = (\" \".join(results)).strip()\n",
    "with open(\"c:/data/text/text_prepared.dat\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(output)\n",
    "\n",
    "print(\"작업완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = word2vec.LineSentence(\"c:/data/text/text_prepared.dat\")\n",
    "model = word2vec.Word2Vec(data, vector_size=100, window=10, hs=1, min_count=2, sg=1)\n",
    "model.save(\"c:/data/text/text_100.model\")\n",
    "print(\"word2vec 모델 생성 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "text = open(\"c:/data/text/ratings.csv\", encoding=\"ms949\").read()\n",
    "wordcloud = WordCloud(font_path = \"c:/Windows/Fonts/malgun.ttf\").generate(text)\n",
    "wordcloud.words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec.load(\"c:/data/text/text_100.model\")\n",
    "print(model.wv.most_similar(positive=[\"영화\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_train :  10000\n",
      "count_test :  10000\n",
      "title_arr size :  20000\n",
      "ctr_arr size :  20000\n",
      "max length :  144\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "title_arr = []\n",
    "ctr_arr = []\n",
    "f = open(\"c:/data/text/ratings_train.csv\", encoding=\"ms949\")\n",
    "reader = csv.reader(f)\n",
    "max_length = 0\n",
    "length = 0\n",
    "max_idx = 0\n",
    "count = 0\n",
    "next(reader, None)\n",
    "count_train = 0\n",
    "for line in reader:\n",
    "    title_arr.append(line[0])\n",
    "    ctr_arr.append(int(line[1]))\n",
    "    length = len(line[0])\n",
    "    if max_length < length:\n",
    "        max_length = length\n",
    "        max_idx = count\n",
    "    count += 1\n",
    "    count_train += 1\n",
    "    if count_train >= 10000: break\n",
    "f.close()\n",
    "\n",
    "f2 = open(\"c:/data/text/ratings_test.csv\", encoding=\"ms949\")\n",
    "reader = csv.reader(f2)\n",
    "next(reader, None)\n",
    "count_test = 0\n",
    "for line in reader:\n",
    "    title_arr.append(line[0])\n",
    "    ctr_arr.append(int(line[1]))\n",
    "    length = len(line[0])\n",
    "    if max_length < length:\n",
    "        max_length = length\n",
    "        max_idx = count\n",
    "    count += 1\n",
    "    count_test += 1\n",
    "    if count_test >= 10000: break\n",
    "f2.close()\n",
    "\n",
    "print(\"count_train : \",count_train)\n",
    "print(\"count_test : \",count_test)\n",
    "print(\"title_arr size : \",len(title_arr))\n",
    "print(\"ctr_arr size : \",len(ctr_arr))\n",
    "print(\"max length : \",max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :  0\n",
      "step :  1000\n",
      "step :  2000\n",
      "step :  3000\n",
      "step :  4000\n",
      "step :  5000\n",
      "step :  6000\n",
      "step :  7000\n",
      "step :  8000\n",
      "step :  9000\n",
      "step :  10000\n",
      "step :  11000\n",
      "step :  12000\n",
      "step :  13000\n",
      "step :  14000\n",
      "step :  15000\n",
      "step :  16000\n",
      "step :  17000\n",
      "step :  18000\n",
      "step :  19000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "title_noun_arr = []\n",
    "for index, title in enumerate(title_arr):\n",
    "    if index % 1000 == 0:\n",
    "        print(\"step : \",index)\n",
    "    title_noun_arr.append(okt.nouns(title))\n",
    "print(len(title_noun_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :  0\n",
      "step :  10000\n",
      "[[array([ 2.9211516 ,  2.07885795,  9.6879388 , -2.06501218,  3.18258903,\n",
      "        3.8852878 , -5.35468854, -4.25787805, -5.54966304, -7.45813175,\n",
      "        0.38150759,  9.82161352,  8.11490294, -0.14043681,  6.92923098,\n",
      "        4.74238594,  9.50121344,  9.55777134,  7.42406869, -5.79672672,\n",
      "       -4.66598523, -5.34374644, -6.63820544,  5.32099849,  9.93356046,\n",
      "       -6.4968982 ,  6.14494617, -3.02361303, -9.25553307, -3.12678824,\n",
      "        6.29089512, -2.41580013,  7.88240341,  3.83309147,  7.08591694,\n",
      "       -5.76136884,  7.35382002, -1.10585173, -2.62531771,  8.50424913,\n",
      "        7.00828959, -7.82105791, -8.188705  ,  7.18123446,  3.98857505,\n",
      "        9.11610603, -2.221377  , -6.70806401,  5.76341386,  4.44447788,\n",
      "        7.08854003, -8.45052209,  6.32465538,  2.7662275 ,  0.5537454 ,\n",
      "       -4.40755091,  2.00725984, -1.83469736,  3.28588288,  4.86423799,\n",
      "       -9.39979609, -6.63301735, -8.87126506,  3.08256869, -2.72688893,\n",
      "        2.04318751, -4.58063358, -5.11944211, -6.20645491,  7.71981717,\n",
      "        4.58214249,  4.67839454,  6.69514676, -8.33405656, -5.46364135,\n",
      "       -3.11418614, -1.83765051, -4.21440487, -3.29819228, -2.19771685,\n",
      "       -1.339706  ,  6.88295167, -9.05994851, -9.17713217,  9.56610851,\n",
      "       -4.72648237,  1.69519291,  1.80895089,  3.4777635 ,  7.69722836,\n",
      "        6.38837751,  7.15581225,  0.20016752,  7.42958195,  2.7238372 ,\n",
      "       -3.872995  , -2.97666961, -1.56779633, -4.56693175, -2.91889342]), array([-3.69999728, -3.25765931, -6.61971053,  9.02551952,  5.56734453,\n",
      "       -4.17404836,  9.01480032,  9.565154  ,  5.04335407,  0.20222991,\n",
      "       -9.68980552, -9.51568207, -8.43396934, -0.72933013, -0.40506072,\n",
      "       -7.47004987,  6.26794989, -0.90318679, -0.22222584, -2.8583188 ,\n",
      "        6.89976245, -0.23629648, -1.96177185,  6.3908295 , -5.52172149,\n",
      "       -3.28534114, -8.13471776, -5.07387102,  1.29693636,  5.40800154,\n",
      "        8.97871924, -9.21684517,  9.04973622, -4.60932559, -8.92532653,\n",
      "       -2.10155706, -0.40304984,  9.44216865,  8.21350942,  6.23277322,\n",
      "       -6.90796059, -8.52583739,  6.00666914,  7.51736868, -1.49189251,\n",
      "       -9.91427432, -8.73087097,  5.98913689,  5.1541626 , -9.97544289,\n",
      "       -5.76182309, -3.99916437,  4.50646052, -0.17864524,  8.70855306,\n",
      "        9.5228736 , -0.17640055,  1.89075768, -5.07933602,  0.56947999,\n",
      "        2.92918819, -3.78349048,  9.09955008,  2.20622742, -6.6509597 ,\n",
      "       -5.45260646,  0.74331746, -8.79690877,  1.60970909,  3.91607493,\n",
      "       -8.11855128,  6.50946517,  6.29752868,  1.09235474, -2.3279284 ,\n",
      "       -0.72998349, -1.91968221,  3.39195355,  0.71002251, -4.74571658,\n",
      "        0.37465167,  9.40407525, -5.43352022, -2.48009079, -1.45623414,\n",
      "       -6.76126894, -6.83566479, -9.4592731 ,  8.13752076,  5.64250952,\n",
      "       -8.62988601,  3.20251192, -6.08518686, -5.81570794,  1.41553095,\n",
      "       -0.10337839,  6.97369457, -8.2427227 , -8.84311534, -7.89498562]), array([ 2.85812778,  1.81528691, -8.85785845, -1.99221066,  1.26604814,\n",
      "        8.99817106,  3.21828509,  1.59374239, -1.3248686 , -3.90980681,\n",
      "        3.22282247,  7.49815241,  7.21093356, -2.88385502, -3.90345097,\n",
      "       -8.88449339, -5.41787448, -3.04089931,  2.24291224,  2.07765011,\n",
      "       -8.47947663, -8.43875346,  3.27884702,  4.0670357 , -1.01962   ,\n",
      "       -1.00196929,  8.43516792,  1.72948603, -2.98383065, -2.76912627,\n",
      "       -3.04602096,  1.40190551,  1.00756237,  0.44422109, -7.70223574,\n",
      "        0.65866305,  5.28995192,  9.85906849, -6.50730337, -1.2272631 ,\n",
      "       -2.51914516, -3.22132638, -7.69597559, -1.83238928,  9.32537256,\n",
      "        1.31356956, -7.54200179, -7.67691878, -0.53363556, -8.0751709 ,\n",
      "       -0.07523458, -6.51460006, -1.55664728, -3.97554296, -1.39181156,\n",
      "        2.72949366, -4.02382881, -2.49870652,  7.3549824 , -7.99695733,\n",
      "       -4.02760823, -6.00802216,  8.65466806,  6.99948232, -6.24701123,\n",
      "       -7.74315067,  4.85266973,  5.34910156, -6.09898964,  5.34648718,\n",
      "        0.25624461,  8.37519048,  8.6411597 , -5.57787542,  0.8898091 ,\n",
      "       -0.93667591,  0.48386152, -4.86207323, -2.75437758,  0.30847626,\n",
      "        8.59245704,  5.26287505, -9.46407675, -1.22695787, -2.72433458,\n",
      "       -0.03934967,  2.73622413, -7.79736645, -5.87654483, -2.6271408 ,\n",
      "        9.54968974, -3.27747738, -0.02437025, -9.21803166,  4.29254883,\n",
      "        1.22242426,  9.68596152,  4.61110374, -5.65221586, -1.7814412 ])]]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "MIN_NOUN_VECTOR_VALUE = -10.0\n",
    "MAX_NOUN_VECTOR_VALUE = 10.0\n",
    "NOUN_VECTOR_SIZE = 100 # 벡터 사이즈 100차원. 위에 만든 모델이 100차원이었으니까 맞춰서\n",
    "\n",
    "def generate_random_noun_vector():\n",
    "    return np.random.uniform(low=MIN_NOUN_VECTOR_VALUE, high=MAX_NOUN_VECTOR_VALUE, size=(NOUN_VECTOR_SIZE,))\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec.load(\"c:/data/text/text_100.model\")\n",
    "title_noun_vector_arr = []\n",
    "for index, title_nouns in enumerate(title_noun_arr):\n",
    "    if index % 10000 == 0:\n",
    "        print(\"step : \", index)\n",
    "    noun_vector_arr = []\n",
    "    for noun in title_nouns:\n",
    "        try:\n",
    "            noun_vector = w2v_model[noun] # word2vec에 단어 입력해서 100차원으로 변환\n",
    "        except Exception as e: # 단어 사전에 없으면 랜덤으로 처리\n",
    "            noun_vector = generate_random_noun_vector()\n",
    "        noun_vector_arr.append(noun_vector)\n",
    "    title_noun_vector_arr.append(noun_vector_arr)\n",
    "\n",
    "print(title_noun_vector_arr[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :  0\n",
      "step :  1000\n",
      "step :  2000\n",
      "step :  3000\n",
      "step :  4000\n",
      "step :  5000\n",
      "step :  6000\n",
      "step :  7000\n",
      "step :  8000\n",
      "step :  9000\n",
      "step :  10000\n",
      "step :  11000\n",
      "step :  12000\n",
      "step :  13000\n",
      "step :  14000\n",
      "step :  15000\n",
      "step :  16000\n",
      "step :  17000\n",
      "step :  18000\n",
      "step :  19000\n",
      "0 37\n"
     ]
    }
   ],
   "source": [
    "max_size = 0\n",
    "min_size = 100\n",
    "avg = 0\n",
    "count = 0\n",
    "sum_size = 0\n",
    "\n",
    "for index, title_noun_vector in enumerate(title_noun_vector_arr):\n",
    "    if len(title_noun_vector) == 0:\n",
    "        pass\n",
    "    sum_size += len(title_noun_vector)\n",
    "    if max_size < len(title_noun_vector):\n",
    "        max_size = len(title_noun_vector)\n",
    "    if min_size > len(title_noun_vector):\n",
    "        min_size = len(title_noun_vector)\n",
    "    count += 1\n",
    "\n",
    "TITLE_LENGTH = max_size\n",
    "\n",
    "def generate_zero_noun_vector(): # 제로패딩 준비\n",
    "    return np.random.uniform(low=0.0, high=0.0, size=(NOUN_VECTOR_SIZE,))\n",
    "\n",
    "title_noun_vector_arr2 = [] # 최대사이즈에 맞게 제로패딩\n",
    "for index, title_noun_vector in enumerate(title_noun_vector_arr):\n",
    "    if index % 1000 == 0:\n",
    "        print('step : ',index)\n",
    "    while len(title_noun_vector) < TITLE_LENGTH:\n",
    "        title_noun_vector.append(generate_zero_noun_vector())\n",
    "    title_noun_vector = title_noun_vector[:TITLE_LENGTH]\n",
    "    title_noun_vector_arr2.append(title_noun_vector)\n",
    "\n",
    "print(min_size, max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9960, 10040]\n",
      "[0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 2\n",
    "ctr_class_arr = []\n",
    "ctr_class_count = [0,0]\n",
    "\n",
    "for index, ctr in enumerate(ctr_arr):\n",
    "    if ctr == 0: # 부정리뷰\n",
    "        ctr_class_arr.append(0.0)\n",
    "        ctr_class_count[0] += 1\n",
    "    elif ctr == 1: # 긍정리뷰\n",
    "        ctr_class_arr.append(1.0)\n",
    "        ctr_class_count[1] += 1\n",
    "    \n",
    "print(ctr_class_count)\n",
    "print(ctr_class_arr[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(arr):\n",
    "    sum1 = 0\n",
    "    for i in arr:\n",
    "        sum1 += i\n",
    "    arr = [float(i)/sum1 for i in arr]\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctr_class_arr 개수 :  20000\n",
      "train 데이터 개수 :  10000\n",
      "train 라벨 개수 :  10000\n",
      "test 데이터 개수 :  10000\n",
      "test 라벨 개수 :  10000\n",
      "train_data_size :  10000\n",
      "test_data_size :  10000\n"
     ]
    }
   ],
   "source": [
    "# 학습용 검증용 나누기\n",
    "\n",
    "test_data_size = count_test\n",
    "train_data_size = count_train\n",
    "\n",
    "train_input = title_noun_vector_arr[0:train_data_size]\n",
    "train_label = ctr_class_arr[0:train_data_size]\n",
    "test_input = title_noun_vector_arr[train_data_size:]\n",
    "test_label = ctr_class_arr[train_data_size:]\n",
    "\n",
    "print(\"ctr_class_arr 개수 : \",len(ctr_class_arr))\n",
    "print(\"train 데이터 개수 : \",len(train_input))\n",
    "print(\"train 라벨 개수 : \",len(train_label))\n",
    "print(\"test 데이터 개수 : \",len(test_input))\n",
    "print(\"test 라벨 개수 : \",len(test_label))\n",
    "print(\"train_data_size : \",train_data_size)\n",
    "print(\"test_data_size : \",test_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras에 넣기 위해 차원 변경\n",
    "\n",
    "train_input = np.array(train_input)\n",
    "train_input = train_input.reshape(train_input.shape[0], train_input.shape[1], NOUN_VECTOR_SIZE, 1)\n",
    "test_input = np.array(test_input)\n",
    "test_input = test_input.reshape(test_input.shape[0], test_input.shape[1], NOUN_VECTOR_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input_a = Input(shape=(train_input.shape[1], NOUN_VECTOR_SIZE, 1), name=\"input-layer\")\n",
    "x = Conv2D(1, (3,3), activation='relu', padding='valid', strides=(1,1))(input_a) # 3*3짜리 필터 한 개, 패딩을 통해 인풋과 아웃풋의 사이즈 똑같게, stride는 쉬프트 값\n",
    "x = Conv2D(1, (3,3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Flatten()(x) # 1차원으로 변환\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "out = Dense(1, activation='sigmoid', name='output-layer')(x)\n",
    "model = Model(inputs=[input_a], outputs=out)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "30/30 [==============================] - 6s 199ms/step - loss: 1.1860 - accuracy: 0.5114 - val_loss: 0.7008 - val_accuracy: 0.5228\n",
      "Epoch 2/5\n",
      "30/30 [==============================] - 5s 151ms/step - loss: 0.7277 - accuracy: 0.4977 - val_loss: 0.6940 - val_accuracy: 0.5112\n",
      "Epoch 3/5\n",
      "30/30 [==============================] - 5s 152ms/step - loss: 0.7020 - accuracy: 0.5057 - val_loss: 0.6935 - val_accuracy: 0.5132\n",
      "Epoch 4/5\n",
      "30/30 [==============================] - 5s 153ms/step - loss: 0.6945 - accuracy: 0.5164 - val_loss: 0.6931 - val_accuracy: 0.5100\n",
      "Epoch 5/5\n",
      "30/30 [==============================] - 5s 152ms/step - loss: 0.6951 - accuracy: 0.5108 - val_loss: 0.6935 - val_accuracy: 0.5096\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(patience=2)\n",
    "hist = model.fit(x=train_input, y=np.array(train_label), validation_split=0.25, batch_size=256, epochs=5, verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 2ms/step - loss: 0.6890 - accuracy: 0.5500\n",
      "loss: 68.90%\n",
      "accuracy: 55.00%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(np.array(test_input)[:100], np.array(test_label)[:100], batch_size=1, verbose=1)\n",
    "print(\"%s: %.2f%%\"%(model.metrics_names[0], scores[0]*100))\n",
    "print(\"%s: %.2f%%\"%(model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "m2 = Model(inputs=model.input, outputs=model.get_layer('output-layer').output)\n",
    "y = m2.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.500761  ],\n",
       "       [0.500761  ],\n",
       "       [0.47888556],\n",
       "       [0.5047337 ],\n",
       "       [0.50270516],\n",
       "       [0.52642244],\n",
       "       [0.48652118],\n",
       "       [0.49697497],\n",
       "       [0.48269248],\n",
       "       [0.49538818]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:/data/text\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"c:/data/text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.500761]]\n",
      "1.0\n",
      "재밌군\n"
     ]
    }
   ],
   "source": [
    "n = 911\n",
    "print(model.predict(np.array(train_input)[n].reshape(1,37,100,1)))\n",
    "print(np.array(train_label)[n])\n",
    "print(title_arr[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn = 시퀀스. 문장의 배열, 단어 순서까지 고려. 얘가 쪼꼼 더 쉬움 ㅎㅎ\n",
    "# cnn = word2vec이 단어 순서를 고려"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cef9e06bb236b2a8629b07e87a04b187b952a0f661eff5533360a155783f0c33"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
